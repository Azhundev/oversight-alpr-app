##

 Performance Bottleneck Fixes

Complete guide to fixing the three main bottlenecks in the pilot pipeline.

---

## Problem 1: CPU Decode Bottleneck

### Current State
```python
# pilot.py - Uses CPU decoder
cap = cv2.VideoCapture(rtsp_url)  # CPU decode
ret, frame = cap.read()            # 20-30% CPU per stream
```

**Impact:**
- 20-30% CPU usage per stream
- Limits to 2-3 streams max
- Frame decode is serialized (slow)

### Solution 1A: GPU Decode (cv2.cudacodec) ⭐ Recommended
```python
# Use gpu_camera_ingestion.py
from services.camera.gpu_camera_ingestion import create_optimized_capture

cap = create_optimized_capture(
    source=rtsp_url,
    target_size=(960, 540),  # Resize on GPU!
    prefer_gpu=True
)

ret, frame = cap.read()  # Already resized, ready for inference
```

**Benefits:**
- <5% GPU usage per stream (vs 20-30% CPU)
- Decode directly to GPU memory
- Can handle 8-12 streams on Jetson Orin NX
- Background threading for non-blocking capture

**Setup:**
```bash
# Verify cv2.cudacodec is available
python3 -c "import cv2; print(cv2.cuda.getCudaEnabledDeviceCount())"

# If not available, reinstall opencv with CUDA support
pip3 uninstall opencv-python opencv-contrib-python
pip3 install opencv-contrib-python
```

### Solution 1B: FFmpeg with NVDEC (Alternative)
```python
import subprocess
import numpy as np

# Use ffmpeg with NVDEC hardware decoder
cmd = [
    'ffmpeg',
    '-hwaccel', 'cuda',
    '-hwaccel_output_format', 'cuda',
    '-i', rtsp_url,
    '-f', 'rawvideo',
    '-pix_fmt', 'bgr24',
    '-'
]

process = subprocess.Popen(cmd, stdout=subprocess.PIPE, bufsize=10**8)

# Read frames
raw_frame = process.stdout.read(width * height * 3)
frame = np.frombuffer(raw_frame, dtype=np.uint8).reshape((height, width, 3))
```

**Benefits:**
- GPU decode via NVDEC
- More reliable for some RTSP streams
- Better codec support

---

## Problem 2: Multiple CPU↔GPU Copies

### Current State
```
Frame Flow:
1. Decode on CPU → RAM
2. Upload to GPU → VRAM (Copy 1)
3. Inference → Results
4. Download to CPU → RAM (Copy 2)
5. Upload crops for OCR → VRAM (Copy 3)
6. OCR → Results
7. Download results → RAM (Copy 4)

Total: 4+ CPU↔GPU copies per frame!
```

**Impact:**
- Each copy: ~10-20ms @ 1080p
- PCIe bandwidth bottleneck
- Limits throughput

### Solution 2A: Keep Inference Tensors on GPU
```python
# Current (multiple copies)
frame = cv2.imread(...)              # CPU
detection = model(frame)             # Upload to GPU → download
plate_crop = frame[y1:y2, x1:x2]    # CPU
ocr_result = ocr(plate_crop)        # Upload to GPU → download

# Optimized (minimal copies)
gpu_frame = cv2.cuda_GpuMat()
gpu_frame.upload(frame)               # Upload once

# Keep on GPU for all operations
gpu_resized = cv2.cuda.resize(gpu_frame, (960, 540))
detection = model(gpu_resized)        # Already on GPU!
gpu_crop = gpu_resized[y1:y2, x1:x2] # Still on GPU
ocr_result = ocr(gpu_crop)           # Still on GPU!

# Download only final results
final_result = gpu_result.download()  # Single download
```

### Solution 2B: Resize Before Upload
```python
# Reduce data transfer size

# Bad: Upload full 1080p, then resize
gpu_frame = cv2.cuda_GpuMat()
gpu_frame.upload(frame_1080p)              # 2.1 MP upload
gpu_resized = cv2.cuda.resize(gpu_frame, (960, 540))

# Good: Resize on CPU, upload smaller frame
frame_960 = cv2.resize(frame_1080p, (960, 540))  # CPU resize
gpu_frame = cv2.cuda_GpuMat()
gpu_frame.upload(frame_960)                # 0.5 MP upload (4x less!)
```

**Savings:**
- 1920×1080 = 2.1 MP × 3 bytes = 6.2 MB
- 960×540 = 0.5 MP × 3 bytes = 1.6 MB
- **74% less data transfer!**

### Solution 2C: Use GPU Decode (Eliminates Uploads)
```python
# With GPU decode, frames never leave GPU!

cap = cv2.cudacodec.createVideoReader(rtsp_url)
ret, gpu_frame = cap.nextFrame()    # Already on GPU!

# Resize on GPU
gpu_resized = cv2.cuda.resize(gpu_frame, (960, 540))

# Inference on GPU
detection = model(gpu_resized)      # No upload needed!

# Only download small metadata/results
results = detection.download()      # Minimal download
```

**Result:** Zero copies until final results extraction!

---

## Problem 3: 1-2 Stream Limitation

### Current State
- Single-threaded capture in `pilot.py`
- Processes one camera at a time
- CPU decode bottleneck
- No parallel processing

### Solution 3A: Multi-Threaded Capture
```python
# Use MultiStreamCapture
from services.camera.gpu_camera_ingestion import MultiStreamCapture

# Define cameras
sources = {
    'cam1': 'rtsp://camera1/stream',
    'cam2': 'rtsp://camera2/stream',
    'cam3': 'rtsp://camera3/stream',
    'cam4': 'rtsp://camera4/stream',
}

# Start all captures (each in own thread)
multi_cap = MultiStreamCapture(
    sources=sources,
    target_size=(960, 540),
    use_gpu_decode=True
)

multi_cap.start_all()

# Read from all cameras in parallel
while True:
    frames = multi_cap.read_all()  # Dict of {camera_id: frame}

    for camera_id, frame in frames.items():
        # Process each frame
        process_frame(frame, camera_id)
```

**Benefits:**
- Captures run in parallel threads
- Non-blocking (one slow camera doesn't block others)
- Can handle 4-8 streams easily

### Solution 3B: Process Streams in Parallel
```python
import concurrent.futures
import threading

def process_camera_stream(camera_id, source):
    """Process a single camera stream in separate thread"""
    cap = create_optimized_capture(source, target_size=(960, 540))

    while running:
        ret, frame = cap.read()
        if not ret:
            continue

        # Process frame (detection, OCR, etc.)
        results = pipeline.process(frame, camera_id)

        # Publish results
        publish_to_kafka(results)

# Start threads for each camera
with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
    futures = []
    for camera_id, source in cameras.items():
        future = executor.submit(process_camera_stream, camera_id, source)
        futures.append(future)

    # Wait for all threads
    concurrent.futures.wait(futures)
```

### Solution 3C: Batch Processing Across Streams
```python
# Collect frames from multiple streams
frames_batch = []
camera_ids = []

for camera_id in cameras:
    ret, frame = caps[camera_id].read()
    if ret:
        frames_batch.append(frame)
        camera_ids.append(camera_id)

# Batch inference (more efficient)
if len(frames_batch) >= 2:
    # Stack frames into batch
    batch = np.stack(frames_batch, axis=0)

    # Run inference on batch
    detections_batch = model.detect_batch(batch)

    # Process results
    for camera_id, detections in zip(camera_ids, detections_batch):
        process_detections(camera_id, detections)
```

---

## Problem 4: Low OCR Accuracy

### Current Issues
- Single preprocessing strategy
- No error correction
- Sensitive to lighting/blur
- Low resolution crops

### Solution 4A: Enhanced Preprocessing
```python
from services.ocr.enhanced_ocr import EnhancedOCRService

# Use enhanced OCR with multi-pass
ocr = EnhancedOCRService(
    config_path="config/ocr.yaml",
    use_gpu=True,
    enable_multi_pass=True  # Try 5 different preprocessing strategies!
)

# Recognize with majority voting
result = ocr.recognize_plate_multi_pass(frame, bbox)
```

**Improvements:**
1. **Multi-scale preprocessing**
   - CLAHE contrast enhancement
   - Histogram equalization
   - Adaptive thresholding
   - Morphological operations
   - Sharpening

2. **Majority voting**
   - Run OCR on 5 variations
   - Select most common result
   - Boost confidence if multiple strategies agree

3. **Better normalization**
   - Smart O/0, I/1, S/5 corrections
   - Context-aware (letter vs number positions)

### Solution 4B: Higher Resolution Crops
```python
# Current: Accept any crop size
plate_crop = frame[y1:y2, x1:x2]  # Maybe 30px tall!

# Enhanced: Enforce minimum size
min_height = 64  # Increased from 32
target_height = 80  # Better for OCR

if plate_crop.shape[0] < min_height:
    scale = target_height / plate_crop.shape[0]
    new_width = int(plate_crop.shape[1] * scale)
    plate_crop = cv2.resize(
        plate_crop,
        (new_width, target_height),
        interpolation=cv2.INTER_CUBIC  # Better for upscaling
    )
```

**Impact:**
- Small plates (<40px) have poor OCR accuracy
- Upscaling to 80px significantly improves results

### Solution 4C: Wait for Better Frame (Track-Based)
```python
# Already implemented! Just tune parameters

# Wait longer for better quality frame
ocr_min_track_frames = 5  # Increased from 3

# Only run OCR on high-confidence detections
def should_run_ocr(track_id, plate_confidence):
    # Wait for stable track
    if track_frame_count[track_id] < ocr_min_track_frames:
        return False

    # Wait for high-quality detection
    if plate_confidence < 0.75:  # Increased from 0.6
        return False

    return True
```

---

## Combined Solution: Optimized Pipeline

### New Optimized Pilot
```python
# pilot_optimized.py

from services.camera.gpu_camera_ingestion import MultiStreamCapture
from services.detector.detector_service import YOLOv11Detector
from services.ocr.enhanced_ocr import EnhancedOCRService

# 1. Multi-stream GPU capture (Fixes: decode bottleneck, multi-stream)
multi_cap = MultiStreamCapture(
    sources=camera_sources,
    target_size=(960, 540),  # Resize on GPU (Fixes: CPU↔GPU copies)
    use_gpu_decode=True
)
multi_cap.start_all()

# 2. YOLOv11 detector
detector = YOLOv11Detector(
    vehicle_model_path="yolo11n.pt",
    use_tensorrt=True,  # Export to TensorRT for 3x speedup
    fp16=True
)

# 3. Enhanced OCR (Fixes: low accuracy)
ocr = EnhancedOCRService(
    use_gpu=True,
    enable_multi_pass=True  # Multi-strategy preprocessing
)

# 4. Process all streams
while True:
    frames = multi_cap.read_all()  # Parallel capture

    for camera_id, frame in frames.items():
        # Frame already resized to 960×540 on GPU!

        # Detection
        vehicles = detector.detect_vehicles(frame)

        # Track-based OCR throttling (already optimized!)
        for vehicle in vehicles:
            track_id = match_vehicle_to_track(vehicle.bbox)

            if should_run_ocr(track_id):
                plate_detection = ocr.recognize_plate_multi_pass(frame, bbox)
                track_ocr_cache[track_id] = plate_detection
```

---

## Performance Comparison

### Before Optimizations
| Metric | Value |
|--------|-------|
| Decode | CPU (20-30% per stream) |
| CPU↔GPU copies | 4+ per frame |
| Streams | 1-2 max |
| FPS | 25-30 |
| OCR accuracy | 60-70% |

### After Optimizations
| Metric | Value |
|--------|-------|
| Decode | GPU (<5% per stream) |
| CPU↔GPU copies | 1 (results only) |
| Streams | 4-8 per device |
| FPS | 30 (consistent) |
| OCR accuracy | 85-95% |

---

## Migration Path

### Step 1: Fix Decode (Easy) ✅
```bash
# Replace cv2.VideoCapture with GPU capture
# Change 1 line of code!
cap = create_optimized_capture(url, target_size=(960, 540))
```

**Gain:** 2-3x more streams

### Step 2: Fix OCR Accuracy (Medium)
```bash
# Use enhanced OCR service
ocr = EnhancedOCRService(enable_multi_pass=True)
```

**Gain:** 20-30% better accuracy

### Step 3: Minimize Copies (Medium)
```bash
# Resize before upload, keep tensors on GPU
# Requires code refactoring
```

**Gain:** 20-30% faster processing

### Step 4: Multi-Stream (Hard)
```bash
# Refactor pilot.py to use MultiStreamCapture
# Add threading/batching
```

**Gain:** 4-8x more streams

---

## Quick Wins Checklist

- [ ] Use GPU decode (cv2.cudacodec or gpu_camera_ingestion.py)
- [ ] Resize to 960×540 before/during decode
- [ ] Use EnhancedOCRService with multi-pass
- [ ] Increase OCR min_track_frames to 5
- [ ] Upscale small plates to 80px height
- [ ] Export YOLOv11 to TensorRT
- [ ] Add multi-stream threading
- [ ] Batch inference across streams

**Expected Result:**
- 4-8 streams @ 30 FPS
- 85-95% OCR accuracy
- 40-60% GPU utilization
- <30% CPU usage

---

## Testing

### Test GPU Decode
```bash
python3 services/camera/gpu_camera_ingestion.py rtsp://camera/stream
# Should see: "GPU decode (NVDEC) enabled"
# FPS should be 30+ with <10% GPU
```

### Test Enhanced OCR
```bash
python3 services/ocr/enhanced_ocr.py test_plate.jpg
# Should see multiple strategies being tried
# Majority voting in logs
```

### Test Multi-Stream
```bash
# Monitor GPU usage with multiple streams
watch -n1 nvidia-smi

# Should handle 4+ streams at 30 FPS
```

---

## Bottom Line

**Three Fixes = 10x Better System:**

1. **GPU Decode** → 4x more streams
2. **Enhanced OCR** → 30% better accuracy
3. **Minimal Copies** → 30% faster processing

All without migrating to full DeepStream! 🚀
