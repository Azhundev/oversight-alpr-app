# Pipeline Architecture Comparison

**Last Updated:** 2025-12-23

This document compares different ALPR pipeline architectures, from the current production-ready distributed system to future DeepStream optimizations.

---

## Current Production System (Distributed Architecture)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPLETE DISTRIBUTED SYSTEM                            â”‚
â”‚                    (Current Production-Ready)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EDGE PROCESSING (pilot.py on Jetson)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RTSP Camera Feed / Video File
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CameraIngestionService â”‚  â—„â”€â”€ Multi-threaded frame capture
â”‚  (cv2.VideoCapture)     â”‚      **GPU hardware decode (NVDEC)** âœ…
â”‚  - RTSP: GPU decode     â”‚      RTSP: 4-6 streams/Jetson (80-90% CPUâ†“)
â”‚  - Video: CPU decode    â”‚      Video files: CPU decode (compatibility)
â”‚  - Frame buffering      â”‚      OpenCV 4.6.0 + GStreamer 1.20.3
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  YOLOv11 Detector       â”‚  â—„â”€â”€ TensorRT FP16 optimized
â”‚  (TensorRT Engine)      â”‚      Vehicle: 10-15ms
â”‚  - Vehicle detection    â”‚      Plate: 5-10ms
â”‚  - Plate detection      â”‚      Total: ~20ms per frame
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ByteTrack Tracker      â”‚  â—„â”€â”€ Multi-object tracking
â”‚  (CPU - Python)         â”‚      Kalman filter + IoU
â”‚  - Track assignment     â”‚      <1ms overhead
â”‚  - Motion prediction    â”‚      Handles occlusions
â”‚  - Track buffering      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PaddleOCR Service      â”‚  â—„â”€â”€ GPU-accelerated OCR
â”‚  (GPU - Per-Track)      â”‚      Per-track throttling
â”‚  - Run ONCE per track   â”‚      10-30ms per plate
â”‚  - Multi-strategy       â”‚      Quality-based filtering
â”‚  - Best-shot selection  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Event Processor        â”‚  â—„â”€â”€ Validation & deduplication
â”‚  (CPU - Python)         â”‚      Fuzzy matching (Levenshtein)
â”‚  - Text normalization   â”‚      5-minute time window
â”‚  - Format validation    â”‚      <1ms per event
â”‚  - Deduplication        â”‚
â”‚  - Metadata enrichment  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka Publisher        â”‚  â—„â”€â”€ Async event publishing
â”‚  (kafka-python)         â”‚      GZIP compression
â”‚  - JSON serialization   â”‚      Idempotent producer
â”‚  - Async with acks      â”‚      5-10ms per message
â”‚  - GZIP compression     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Visualization          â”‚  â—„â”€â”€ Real-time display
â”‚  (CPU - OpenCV)         â”‚      Bboxes, track IDs, text
â”‚  - Display rendering    â”‚      Optional (headless mode)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MESSAGING LAYER (Docker on same Jetson or separate server)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Apache Kafka Broker    â”‚  â—„â”€â”€ Event streaming
â”‚  (Confluent CP 7.5.0)   â”‚      Topic: alpr.plates.detected
â”‚  - Message buffering    â”‚      10,000+ msg/s capacity
â”‚  - Partitioning         â”‚      7-day retention
â”‚  - Consumer groups      â”‚      GZIP compression
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                          â”‚
    â–¼                                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka UI               â”‚        â”‚  Other Consumers â”‚
â”‚  (Web Interface)        â”‚        â”‚  - Analytics     â”‚
â”‚  - Topic monitoring     â”‚        â”‚  - Alerts        â”‚
â”‚  - Message inspection   â”‚        â”‚  - Dashboards    â”‚
â”‚  - Consumer lag         â”‚        â”‚  - Integrations  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka Consumer Service â”‚  â—„â”€â”€ Event persistence
â”‚  (Python Service)       â”‚      100-500 events/s
â”‚  - Message consumption  â”‚      Graceful shutdown
â”‚  - JSON deserialization â”‚      <10ms per event
â”‚  - Offset management    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
STORAGE LAYER (Docker on same Jetson or separate server)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Storage Service        â”‚  â—„â”€â”€ Database abstraction
â”‚  (Python + psycopg2)    â”‚      Connection pooling
â”‚  - Connection pooling   â”‚      Duplicate prevention
â”‚  - Batch inserts        â”‚      1-5ms per insert
â”‚  - Error handling       â”‚      500-1000 inserts/s
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TimescaleDB (PostgreSQL)   â”‚  â—„â”€â”€ Time-series database
â”‚  (PostgreSQL 16 + Timescale)â”‚      Hypertable partitioning
â”‚  - Hypertable partitioning  â”‚      Automatic indexes
â”‚  - Time-based chunks        â”‚      Continuous aggregates
â”‚  - Automatic compression    â”‚      1-2GB RAM usage
â”‚  - Retention policies       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
API LAYER (Docker on same Jetson or separate server)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query API Service      â”‚  â—„â”€â”€ REST API (FastAPI)
â”‚  (FastAPI + Uvicorn)    â”‚      50-100 req/s
â”‚  - Multiple endpoints   â”‚      10-100ms latency
â”‚  - Pagination support   â”‚      Connection pooling
â”‚  - CORS enabled         â”‚      OpenAPI docs
â”‚  - Health checks        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
Client Applications
- Web dashboards
- Mobile apps
- Analytics tools
- Alert systems
- Third-party integrations

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PERFORMANCE METRICS (Complete System)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EDGE PROCESSING:
  Streams per Jetson Orin NX:  4-6 RTSP (with GPU decode + OCR) or 1-2 video files
  FPS per stream:              15-25 FPS (full pipeline)
  End-to-end edge latency:     40-90ms (with OCR), 20-40ms (detection only)
  CPU usage (RTSP GPU):        15-25% (with TensorRT + GPU decode)
  CPU usage (Video CPU):       40-60% (with TensorRT, CPU decode)
  GPU usage:                   30-50%
  Video decode:                GPU (RTSP), CPU (video files)
  Events published:            1-10 events/minute per camera

BACKEND SERVICES:
  Kafka throughput:            10,000+ messages/second
  Storage throughput:          500-1000 inserts/second
  API throughput:              50-100 requests/second
  Total system capacity:       100+ events/second sustained

DEPLOYMENT OPTIONS:
  All-in-one (Jetson):         All services on single Jetson
  Distributed:                 Edge on Jetson, backend on server
  Multi-edge:                  Multiple Jetsons â†’ shared backend

RESOURCE USAGE (Docker Backend):
  Kafka Broker:                512MB RAM, <10% CPU
  Kafka Consumer:              256MB RAM, <5% CPU
  TimescaleDB:                 1-2GB RAM, 10-20% CPU
  Query API:                   256MB RAM, <5% CPU
  Total Backend:               ~2-3GB RAM, ~30% CPU
```

---

## Production DeepStream Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DEEPSTREAM PIPELINE                         â”‚
â”‚                 (Production - Future)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

RTSP Camera Feed (Stream 1, 2, 3, 4...)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   uridecodebin         â”‚  â—„â”€â”€ Auto-detects codec
â”‚   (DeepStream)         â”‚      RTSP source handling
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   NVDEC                â”‚  â—„â”€â”€ GPU Hardware Decoder
â”‚   (GPU H.264/H.265)    â”‚      <5% GPU per stream
â”‚                        â”‚      Decode directly to GPU memory!
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   nvstreammux          â”‚  â—„â”€â”€ Batch multiple streams
â”‚   (GPU Batching)       â”‚      Combine 4-8 streams into one batch
â”‚                        â”‚      No CPU involvement
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   nvvideoconvert       â”‚  â—„â”€â”€ GPU-accelerated resize
â”‚   (GPU Resize)         â”‚      1920Ã—1080 â†’ 960Ã—540
â”‚                        â”‚      Stays on GPU!
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   nvinfer (Primary GIE)     â”‚  â—„â”€â”€ TensorRT Engine
â”‚   YOLOv11 Vehicle+Plate     â”‚      Batch inference (4-8 frames)
â”‚   (TensorRT FP16)           â”‚      ~8-12ms per frame
â”‚   - .engine file            â”‚      2.5-3x faster than PyTorch!
â”‚   - Optimized kernels       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   nvtracker            â”‚  â—„â”€â”€ NvDCF Multi-Object Tracker
â”‚   (GPU Tracking)       â”‚      GPU-accelerated
â”‚   - NvDCF algorithm    â”‚      Handles occlusions
â”‚   - Re-identification  â”‚      Batch processing
â”‚   - Kalman filtering   â”‚      5-10x faster than CPU
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Python Probe         â”‚  â—„â”€â”€ Extract metadata, run OCR
â”‚   (Custom Callback)    â”‚      Your throttling logic here!
â”‚   - Track-based OCR    â”‚
â”‚   - should_run_ocr()   â”‚      Same optimization principles
â”‚   - Cache results      â”‚      Run ONCE per track
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   nvdsanalytics (Optional)  â”‚  â—„â”€â”€ Zone crossing, line counting
â”‚   (GPU Analytics)           â”‚      Built-in analytics module
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   nvdsosd              â”‚  â—„â”€â”€ GPU On-Screen Display
â”‚   (GPU Rendering)      â”‚      Draw bboxes, text on GPU
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   nvmsgconv            â”‚  â—„â”€â”€ Convert metadata to JSON
â”‚   (Message Converter)  â”‚      Schema: Kafka, MQTT, AMQP
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   nvmsgbroker          â”‚  â—„â”€â”€ Publish events
â”‚   (Kafka/MQTT)         â”‚      Built-in message broker
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
Kafka Topic / MQTT Broker

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PERFORMANCE METRICS (DeepStream)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Streams per Jetson Orin NX:  8-12
FPS per stream:              30
End-to-end latency:          30-50ms
CPU usage:                   20-30%
GPU usage:                   70-90%
CPUâ†”GPU copies:              0 (zero-copy!)
Memory bandwidth:            LOW (everything on GPU)
```

---

## Side-by-Side Feature Comparison

| Feature | Current System | Future DeepStream |
|---------|----------------|-------------------|
| **Architecture** | Distributed (Edge + Backend) | Distributed (Edge + Backend) |
| **Video Decode (RTSP)** | **GPU (NVDEC) via GStreamer** âœ… | GPU (NVDEC) |
| **Video Decode (Files)** | CPU (compatibility) | N/A (production uses RTSP) |
| **Decode Overhead (RTSP)** | **<5% GPU** âœ… | <5% GPU |
| **Decode Overhead (Files)** | 15-25% CPU | N/A |
| **Resize** | CPU | GPU |
| **Inference** | TensorRT FP16 | TensorRT FP16 |
| **Inference Time** | 20ms (vehicle + plate) | 8-12ms (batched, 2x faster) |
| **Tracking** | ByteTrack (CPU) | NvDCF (GPU) |
| **OCR** | PaddleOCR + Per-Track Throttling | Same (Python probe) |
| **Event Processing** | âœ… Full (validation + dedup) | âœ… Same |
| **Message Broker** | âœ… Kafka (async) | âœ… Kafka (nvmsgbroker) |
| **Storage** | âœ… TimescaleDB | âœ… Same |
| **Query API** | âœ… FastAPI (REST) | âœ… Same |
| **Pipeline** | Sequential (GPU decode RTSP) | Batched + Zero-copy |
| **CPUâ†”GPU Copies (RTSP)** | 1-2 per frame | 0 (zero-copy) |
| **CPUâ†”GPU Copies (Files)** | 2-4 per frame | N/A |
| **Streams/Device (RTSP)** | **4-6 (with OCR)** âœ… | 8-12 (2x more) |
| **Streams/Device (Files)** | 1-2 (with OCR) | N/A |
| **Edge Latency** | 40-90ms | 30-50ms (2x faster) |
| **End-to-End** | Edge â†’ Kafka â†’ DB â†’ API | Same |
| **Development Speed** | âš¡ Fast | Moderate |
| **Production Ready** | âœ… Yes (with backend) | âœ… Yes (higher throughput) |
| **Cost/Complexity** | Medium | Higher |

---

## Memory Flow Comparison

### Pilot Pipeline Memory Flow
```
RAM (CPU)  â†â†’  VRAM (GPU)  â†â†’  RAM (CPU)  â†â†’  VRAM (GPU)
   â†“              â†“              â†“              â†“
Decode         Vehicle        Track          OCR
              Detection      (CPU)       Recognition
   â†“              â†“              â†“              â†“
 Copy 1        Copy 2        Copy 3        Copy 4
(Upload)      (Download)     (Upload)     (Download)

Total PCIe Bandwidth: ~2-4 GB/s per stream @ 1080p30
Bottleneck: PCIe bus, memory copies
```

### DeepStream Memory Flow
```
VRAM (GPU) â†’ VRAM (GPU) â†’ VRAM (GPU) â†’ RAM (Python) â†’ VRAM (GPU)
   â†“            â†“            â†“             â†“              â†“
Decode     Detection    Tracking    Track Cache      OCR
(NVDEC)    (TensorRT)   (NvDCF)     (Metadata)   (PaddleOCR)
   â†“            â†“            â†“             â†“              â†“
ZERO COPY  ZERO COPY   ZERO COPY    Metadata      ZERO COPY
                                      only!

Total PCIe Bandwidth: ~100-200 MB/s (just metadata)
Benefit: 10-20x less memory bandwidth
```

---

## OCR Throttling: Same in Both!

### Critical Point: Track-Based Optimization Works Everywhere

**Pilot (Current):**
```python
# pilot.py - Line 255
if self.should_run_ocr(track_id):  # Once per track!
    ocr_result = self.ocr.recognize_plate(frame, bbox)
    self.track_ocr_cache[track_id] = ocr_result  # Cache it
```

**DeepStream (Future):**
```python
# deepstream_probe.py
def ocr_probe_callback(pad, info, user_data):
    obj_meta = frame_meta.obj_meta_list
    track_id = obj_meta.object_id  # From NvDCF tracker

    if should_run_ocr(track_id):  # Same logic!
        ocr_result = ocr_service.recognize_plate(...)
        track_ocr_cache[track_id] = ocr_result  # Same cache!
```

**Key Insight:**
- Your optimization logic (run once per track) is **platform-agnostic**
- Works in pure Python pilot
- Works in DeepStream production
- Same 10-30x performance gain!

---

## Evolution Timeline

### Phase 1: Pilot Development (Completed âœ…)
- **Purpose:** Algorithm development, OCR testing
- **Status:** Production-ready for 1-2 streams
- **Features:**
  - Pure Python pipeline (pilot.py)
  - PyTorch inference
  - Simple tracking
  - Local CSV logging
- **Throughput:** 1-2 streams @ 25-30 FPS

### Phase 2: Distributed Architecture (Current âœ…)
- **Purpose:** Scalable production deployment
- **Status:** Production-ready with complete backend + GPU optimization
- **Features:**
  - **GPU hardware video decode (NVDEC) for RTSP** âœ…
  - TensorRT FP16 optimization
  - ByteTrack multi-object tracking
  - Per-track OCR throttling
  - Event validation & deduplication
  - **Kafka message broker**
  - **TimescaleDB storage**
  - **REST API (FastAPI)**
  - Docker-based backend services
- **Throughput:** 4-6 RTSP streams @ 15-25 FPS (edge), 100+ events/s (backend)
- **Deployment:** All-in-one or distributed
- **Optimization:** OpenCV 4.6.0 with GStreamer 1.20.3

### Phase 3: DeepStream Optimization (Future)
- **Purpose:** Maximum throughput for multi-camera deployments
- **Timeline:** When scaling to 5+ streams per device
- **Changes:**
  1. Replace `pilot.py` with DeepStream app
  2. GPU video decode (NVDEC)
  3. Zero-copy GPU pipeline
  4. GPU-accelerated tracking (NvDCF)
  5. Keep OCR in Python probes (same throttling logic)
  6. Replace Kafka Publisher with nvmsgbroker
  7. **Keep all backend services (Kafka, Storage, API) unchanged**
- **Throughput:** 8-12 streams @ 30 FPS (edge), same backend

### Phase 4: Full Optimization (Optional)
- **Purpose:** Extreme scale (100+ cameras)
- **Timeline:** Enterprise deployment
- **Changes:**
  1. Multi-GPU support
  2. OCR in Triton Inference Server (optional)
  3. Full C++ DeepStream app
  4. Horizontal scaling (multiple Jetsons)
  5. Load balancing across edges
  6. Database sharding (if needed)

---

## When to Migrate to DeepStream?

### Current System is Good For:
- âœ… **4-6 RTSP camera streams per Jetson** (with GPU decode) âœ…
- âœ… 1-2 video file streams per Jetson
- âœ… Development and testing
- âœ… Production deployments (with backend)
- âœ… Rapid feature iteration
- âœ… Complete event persistence and querying
- âœ… Multi-edge deployments (multiple Jetsons)
- âœ… Budget-conscious deployments
- âœ… Small to medium scale (10-30 cameras total)

### Consider DeepStream Migration When:
- âœ… Need 8+ streams per single Jetson device (current: 4-6)
- âœ… Latency critical (<30ms edge processing, current: 40-90ms)
- âœ… GPU utilization must be maximized beyond current 30-50%
- âœ… Integration with NVIDIA Metropolis required
- âœ… Hardware video encoding needed (recording)
- âœ… Willing to invest in C++/GStreamer development
- âœ… Need zero-copy GPU pipeline (current: 1-2 copies for RTSP)
- âœ… Large scale deployment (50+ cameras total)

---

## Bottom Line

### Current Distributed System (Phase 2) âœ…
**Production-Ready Features:**
- âœ… Complete edge processing with TensorRT optimization
- âœ… **GPU hardware video decode (NVDEC) for RTSP streams** âœ…
- âœ… ByteTrack multi-object tracking
- âœ… Per-track OCR throttling (10-30x performance gain)
- âœ… Event validation and deduplication
- âœ… Kafka message broker for async streaming
- âœ… TimescaleDB for time-series storage
- âœ… REST API for event querying
- âœ… Docker-based backend services
- âœ… All-in-one or distributed deployment options
- âœ… Scalable to multiple edge devices

**Suitable For:**
- Small to medium deployments (10-30 cameras total)
- **4-6 RTSP streams per Jetson Orin NX** (with GPU decode + OCR) âœ…
- 1-2 video file streams per Jetson Orin NX
- Complete event lifecycle (capture â†’ storage â†’ query)
- Budget-conscious projects
- Rapid development and iteration

**Performance:**
- **RTSP:** 80-90% CPU reduction vs CPU decode, 3x stream capacity increase
- **Video files:** CPU decode for compatibility (looping/seeking)

### Future DeepStream System (Phase 3)
**Advantages Over Current:**
- 2x more streams per device (8-12 vs 4-6 for RTSP)
- 1.5x lower edge latency (30-50ms vs 40-90ms)
- Zero-copy GPU pipeline (vs 1-2 copies for RTSP)
- GPU-accelerated tracking (vs CPU ByteTrack)
- Batched inference across multiple streams

**Same As Current:**
- âœ… Kafka + TimescaleDB + REST API backend
- âœ… Per-track OCR throttling logic
- âœ… Event processing and deduplication
- âœ… Complete event lifecycle

**Trade-offs:**
- Higher development complexity (C++/GStreamer)
- Longer development time
- More difficult to debug
- Higher learning curve

---

## Deployment Patterns

### Pattern 1: All-in-One (Single Jetson)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Jetson Orin NX (32GB)          â”‚
â”‚                                        â”‚
â”‚  pilot.py (Edge Processing)            â”‚
â”‚  + Docker Services (Backend)           â”‚
â”‚    - Kafka Broker                      â”‚
â”‚    - Kafka Consumer                    â”‚
â”‚    - TimescaleDB                       â”‚
â”‚    - Query API                         â”‚
â”‚                                        â”‚
â”‚  Capacity: 1-2 cameras                 â”‚
â”‚  RAM Usage: 12-16GB total              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
**Best For:** Single location, 1-2 cameras, simple deployment

**Pros:** Simple, single device, easy to manage
**Cons:** Limited scalability, single point of failure

---

### Pattern 2: Edge + Shared Backend
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Jetson #1   â”‚       â”‚  Jetson #2   â”‚
â”‚  (Edge Only) â”‚       â”‚  (Edge Only) â”‚
â”‚              â”‚       â”‚              â”‚
â”‚  pilot.py    â”‚       â”‚  pilot.py    â”‚
â”‚  1-2 cameras â”‚       â”‚  1-2 cameras â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                      â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   Backend Server (Ubuntu)     â”‚
       â”‚   + Docker Services           â”‚
       â”‚     - Kafka Broker            â”‚
       â”‚     - Kafka Consumer          â”‚
       â”‚     - TimescaleDB             â”‚
       â”‚     - Query API               â”‚
       â”‚                               â”‚
       â”‚   Capacity: 10-20 cameras     â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
**Best For:** Multiple locations, 4-20 cameras, centralized backend

**Pros:** Scalable (add more Jetsons), centralized data, easier maintenance
**Cons:** Network dependency, requires separate server

---

### Pattern 3: Multi-Site Distributed
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SITE 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SITE 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            â”‚    â”‚                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”      â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚Jetsonâ”‚   â”‚Jetsonâ”‚      â”‚    â”‚  â”‚Jetsonâ”‚   â”‚Jetsonâ”‚      â”‚
â”‚  â”‚ #1   â”‚   â”‚ #2   â”‚      â”‚    â”‚  â”‚ #3   â”‚   â”‚ #4   â”‚      â”‚
â”‚  â””â”€â”€â”€â”¬â”€â”€â”˜   â””â”€â”€â”€â”¬â”€â”€â”˜      â”‚    â”‚  â””â”€â”€â”€â”¬â”€â”€â”˜   â””â”€â”€â”€â”¬â”€â”€â”˜      â”‚
â”‚      â”‚          â”‚         â”‚    â”‚      â”‚          â”‚         â”‚
â”‚      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â”‚    â”‚      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â”‚
â”‚           â”‚               â”‚    â”‚           â”‚               â”‚
â”‚      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”          â”‚    â”‚      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”          â”‚
â”‚      â”‚ Local   â”‚          â”‚    â”‚      â”‚ Local   â”‚          â”‚
â”‚      â”‚ Backend â”‚          â”‚    â”‚      â”‚ Backend â”‚          â”‚
â”‚      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜          â”‚    â”‚      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Central Backend  â”‚
                â”‚  (Kafka Mirror)   â”‚
                â”‚  (Global DB)      â”‚
                â”‚  (Analytics)      â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
**Best For:** Enterprise, 20+ cameras, multiple sites

**Pros:** Geo-distributed, redundant, local + global analytics
**Cons:** Complex, requires Kafka mirroring, higher cost

---

## Scalability Strategy

### Horizontal Scaling (Current System)
**Add More Jetson Devices:**

| Jetsons | Total Cameras | Backend Requirements |
|---------|---------------|----------------------|
| 1 | 1-2 | 4GB RAM, 2 CPU cores |
| 2-5 | 2-10 | 8GB RAM, 4 CPU cores |
| 6-10 | 12-20 | 16GB RAM, 8 CPU cores |
| 11-20 | 22-40 | 32GB RAM, 16 CPU cores |

**Backend scales independently of edge devices!**

### Vertical Scaling (Future DeepStream)
**More Cameras Per Jetson:**

| System | Cameras/Jetson | Total Jetsons Needed (20 cameras) |
|--------|----------------|-----------------------------------|
| Current (pilot.py with GPU decode) | **4-6** âœ… | **4-5 Jetsons** |
| DeepStream | 8-12 | 2 Jetsons |

**DeepStream reduces hardware costs for large deployments.**

---

## Key Takeaway

**Your optimization work is platform-agnostic! ğŸ¯**

The track-based OCR throttling, event validation, and distributed architecture you've built will work with:
- âœ… Current Python pipeline (pilot.py)
- âœ… Future DeepStream pipeline
- âœ… Any edge processing framework

**The backend services (Kafka, Storage, API) remain unchanged regardless of edge implementation.**

You've built a production-ready system that can scale horizontally (more Jetsons) now, and vertically (more streams per Jetson) later with DeepStream migration.
